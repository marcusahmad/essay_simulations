{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096caf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import norm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5326c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EIF(X, Y, confidence_level, true_feature_importance, base_learner):\n",
    "    feature_of_interest = 0\n",
    "    \n",
    "    K = 10 # will split data into 2K folds\n",
    "    \n",
    "    X_df = pd.DataFrame(X)\n",
    "    Y_df = pd.DataFrame(Y)\n",
    "    coverage = 0\n",
    "    splitting_index = [random.randint(0,2*K-1) for i in range(n)]\n",
    "    X_df['splitting_index'] = splitting_index\n",
    "    Y_df['splitting_index'] = splitting_index\n",
    "    v_cumulative = 0\n",
    "    v_LOCO_cumulative = 0\n",
    "    v_dot_cumulative = 0\n",
    "    v_dot_LOCO_cumulative = 0\n",
    "    n_s = 0\n",
    "    for k in range(2*K):\n",
    "        X_train_df = X_df[X_df['splitting_index'] != k]\n",
    "        X_test_df = X_df[X_df['splitting_index'] == k]\n",
    "        Y_train_df = Y_df[Y_df['splitting_index'] != k]\n",
    "        Y_test_df = Y_df[Y_df['splitting_index'] == k]\n",
    "        Y_bar = Y_test_df.mean()\n",
    "        S_Y = Y_test_df.var()\n",
    "        if k < K:\n",
    "            regression_function = base_learner.fit(X_train_df.drop('splitting_index',axis=1),Y_train_df)\n",
    "            tmp = (Y_test_df - regression_function.predict(X_test_df.drop('splitting_index',axis=1)))**2\n",
    "            tmp_2 = (Y_test_df-Y_bar)**2\n",
    "            S = tmp.mean()\n",
    "            v_dot = ((tmp/S_Y - S/S_Y*tmp_2/S_Y)**2).mean()\n",
    "            v = 1-S/S_Y\n",
    "            v_cumulative += v\n",
    "            v_dot_cumulative += v_dot\n",
    "        if k >= K:\n",
    "            LOCO_regression_function = base_learner.fit(X_train_df.drop([feature_of_interest,'splitting_index'],axis=1),Y_train_df)\n",
    "            tmp = (Y_test_df - LOCO_regression_function.predict(X_test_df.drop([feature_of_interest,'splitting_index'],axis=1)))**2\n",
    "            tmp_2 = (Y_test_df-Y_bar)**2\n",
    "            S = tmp.mean()\n",
    "            v_dot = ((tmp/S_Y - S/S_Y*tmp_2/S_Y)**2).mean()\n",
    "            v = 1-S/S_Y\n",
    "            v_LOCO_cumulative += v\n",
    "            v_dot_LOCO_cumulative += v_dot\n",
    "            n_s += len(Y_test_df)\n",
    "    estimator = ((v_cumulative-v_LOCO_cumulative)/K)[0]\n",
    "    width_multiplier = (((v_dot_cumulative/(n-n_s)+v_dot_LOCO_cumulative/n_s)/K)[0])**0.5\n",
    "    CI = (estimator-norm.ppf(1-(1-confidence_level)/2)*width_multiplier,estimator+norm.ppf(1-(1-confidence_level)/2)*width_multiplier)\n",
    "    \n",
    "    # evaluate coverage\n",
    "    if (true_feature_importance >= CI[0]) and (true_feature_importance <= CI[1]):\n",
    "        coverage += 1\n",
    "    return(coverage,width_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f65f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Floodgate(X, Y, confidence_level, true_feature_importance, correlation, base_learner): #confidence_level needs to be greater than 0.5\n",
    "    feature_of_interest = 0\n",
    "    feature_importance = true_feature_importance # true feature importance for evaluation of miscoverage rate\n",
    "    \n",
    "    miscoverage = 0\n",
    "    \n",
    "# split data for fitting working regression function vs inference\n",
    "    X_regress = X[:int(n/2)]\n",
    "    Y_regress = Y[:int(n/2)]\n",
    "    X_inference = X[int(n/2):]\n",
    "    Y_inference = Y[int(n/2):]\n",
    "    \n",
    "# fit the working regression function\n",
    "    working_model = base_learner.fit(X_regress,Y_regress)\n",
    "\n",
    "    number_of_features = X_regress.shape[1]\n",
    "    feature_of_interest_bool = np.zeros((number_of_features,))\n",
    "    feature_of_interest_bool[feature_of_interest] += 1\n",
    "    feature_of_interest_bool = (feature_of_interest_bool>0)\n",
    "    \n",
    "# MC re-sampling to estimate expectations in algorithm\n",
    "    K = 10\n",
    "    X_MC = np.zeros((1,number_of_features))\n",
    " # concatenate re-samples for MC in order to vectorise computing the output of working_model on them\n",
    "    for i in range(len(Y_inference)):\n",
    "        feature_MC = np.random.normal(correlation*X_inference[i,1],(1-correlation**2)**0.5,(K,))\n",
    "        tmp = np.zeros((K,number_of_features))\n",
    "        tmp[:,feature_of_interest] += feature_MC\n",
    "        tmp[:,~feature_of_interest_bool] += X_inference[i,~feature_of_interest_bool]\n",
    "        X_MC = np.concatenate((X_MC,tmp), axis = 0)\n",
    "    X_MC = X_MC[1:]\n",
    "    Y_MC = working_model.predict(X_MC)\n",
    "    Y_MC = Y_MC.reshape((-1,K)).T\n",
    "    Y_MC_means = Y_MC.mean(axis=0)\n",
    "\n",
    "    # compute LCB\n",
    "    R = Y_inference*(working_model.predict(X_inference)-Y_MC_means)\n",
    "    V = Y_MC.var(axis=0)*(K/(K-1))\n",
    "    Y_inference_squared = Y_inference**2\n",
    "    R_bar = R.mean()\n",
    "    V_bar = V.mean()\n",
    "    Y_bar = Y_inference.mean()\n",
    "    Y_squared_bar = Y_inference_squared.mean()\n",
    "    cov = np.cov(np.array([R,V,Y_inference,Y_inference_squared]))*(Y_inference.shape[0]/(Y_inference.shape[0]-1))\n",
    "    tmp_var = Y_squared_bar - Y_bar**2\n",
    "    s = 4*cov[0,0] + R_bar**2/V_bar**2*cov[1,1] + R_bar**2/tmp_var**2*cov[2,2]+4*R_bar**2*Y_bar**2/tmp_var**2*cov[3,3]\n",
    "    s = s - 4*R_bar/V_bar*cov[0,1] - 4*R_bar/tmp_var*cov[0,2] + 8*R_bar*Y_bar/tmp_var*cov[0,3]\n",
    "    s = s + 2*R_bar**2/V_bar/tmp_var*cov[1,2] - 4*R_bar**2*Y_bar/V_bar/tmp_var*cov[1,3] - 4*R_bar**2*Y_bar/tmp_var**2*cov[2,3]\n",
    "    s = s*R_bar**2/V_bar**2/tmp_var**2\n",
    "    s = s**0.5\n",
    "\n",
    "    width_multiplier = s/(len(Y_inference))**0.5\n",
    "    LCB = max(R_bar**2/V_bar/tmp_var-norm.ppf(confidence_level)*width_multiplier,0)\n",
    "\n",
    "# evaluate coverage\n",
    "    if (feature_importance < LCB):\n",
    "        miscoverage += 1\n",
    "    return(1-miscoverage,width_multiplier,LCB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6ebcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlated data covariance matrix\n",
    "def correlated_cov_matrix(number_of_features,correlation):\n",
    "    cov = np.eye(number_of_features)\n",
    "    for i in range(number_of_features-1):\n",
    "        cov[i,i+1] = correlation\n",
    "        cov[i+1,i] = correlation\n",
    "    return(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79407c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "iterations = 100\n",
    "confidence_level = 0.8 # needs to be greater than 0.5 to satisfy floodgate's conditions\n",
    "base_learner = RandomForestRegressor(n_estimators = 1000) # LinearRegression() RandomForestRegressor(n_estimators = 1000) RidgeCV(cv=5)\n",
    "                             # need to change base_learner in minipatch directly\n",
    "noise_variance = 1\n",
    "correlation = 0 # need abs(correlation) < 2/(1+root(5)) = 0.618 for positive definite correlation matrix\n",
    "coefficient = 1\n",
    "\n",
    "true_feature_importance = 0 # 0.3249462191 coefficient**2*(1-correlation**2)/(2+2*(coefficient-1)*correlation+coefficient**2+noise_variance)\n",
    "\n",
    "cumulative_coverages = {'EIF': 0, 'Floodgate': 0}\n",
    "cumulative_width_multipliers = {'EIF': 0, 'Floodgate': 0}\n",
    "\n",
    "cumulative_floodgate_overshot = {'Floodgate': 0}\n",
    "overshot_counter = 0\n",
    "zero_LCB_counter = 0\n",
    "for iteration in range(iterations):\n",
    "    print(iteration)\n",
    "    \n",
    "    # generate data\n",
    "    n = 2500\n",
    "    mean = np.zeros(4)\n",
    "    data_cov = correlated_cov_matrix(4,correlation)\n",
    "    X = np.random.multivariate_normal(mean,data_cov,n)\n",
    "    noise = np.random.normal(0,noise_variance**0.5,n)\n",
    "    Y = 1.5*np.sin(X[:,1]*X[:,2]) + 0.25*np.exp(X[:,3]) + noise\n",
    "#     Y = coefficient*X[:,0]+1*X[:,1]-1*X[:,2] + noise\n",
    "\n",
    "    EIF_output = EIF(X,Y,confidence_level,true_feature_importance,base_learner)\n",
    "    Floodgate_output = Floodgate(X,Y,confidence_level,true_feature_importance,correlation,base_learner)\n",
    "\n",
    "    cumulative_coverages['EIF'] += EIF_output[0]\n",
    "    cumulative_coverages['Floodgate'] += Floodgate_output[0]\n",
    "\n",
    "    cumulative_width_multipliers['EIF'] += EIF_output[1]\n",
    "    cumulative_width_multipliers['Floodgate'] += Floodgate_output[1]\n",
    "\n",
    "    if Floodgate_output[2] == 0:\n",
    "        zero_LCB_counter += 1\n",
    "    if Floodgate_output[0] == 1:\n",
    "        cumulative_floodgate_overshot['Floodgate'] += true_feature_importance - Floodgate_output[2]\n",
    "        overshot_counter += 1\n",
    "if overshot_counter != 0:\n",
    "    avg_overshot = cumulative_floodgate_overshot['Floodgate']/overshot_counter\n",
    "else:\n",
    "    avg_overshot = np.nan\n",
    "print('EIF coverage is ' + str(cumulative_coverages['EIF']/iterations))\n",
    "print('Floodgate coverage is ' + str(cumulative_coverages['Floodgate']/iterations))\n",
    "print('EIF average width multiplier is ' + str(cumulative_width_multipliers['EIF']/iterations))\n",
    "print('Floodgate average width multiplier is ' + str(cumulative_width_multipliers['Floodgate']/iterations))\n",
    "print('Floodgate average overshot is ' + str(avg_overshot))\n",
    "print(zero_LCB_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c342f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 'for' loop to evaluate performance over varied correlation results \n",
    "\n",
    "iterations = 1000\n",
    "confidence_level = 0.8 # needs to be greater than 0.5 to satisfy floodgate's conditions\n",
    "base_learner = RidgeCV(cv=5) #  RandomForestRegressor(n_estimators = 1000) RidgeCV(cv=5) LinearRegression()\n",
    "                                 # need to change base_learner in minipatch directly\n",
    "correlations =  [0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6]\n",
    "results_true_feature_importance = []\n",
    "\n",
    "results_EIF_coverage_mean = []\n",
    "results_Floodgate_coverage_mean = []\n",
    "results_EIF_width_multiplier_mean = []\n",
    "results_Floodgate_width_multiplier_mean = []\n",
    "results_Floodgate_overshot_mean = []\n",
    "results_zero_LCB_counter_mean = []\n",
    "\n",
    "results_EIF_coverage_se = []\n",
    "results_Floodgate_coverage_se = []\n",
    "results_EIF_width_multiplier_se = []\n",
    "results_Floodgate_width_multiplier_se = []\n",
    "results_Floodgate_overshot_se = []\n",
    "results_zero_LCB_counter_se = []\n",
    "\n",
    "for correlation in correlations:\n",
    "    print(correlation)\n",
    "    \n",
    "    noise_variance = 1\n",
    "#     correlation = 0.15 # need abs(correlation) < 2/(1+root(5)) = 0.618 for positive definite correlation matrix\n",
    "    coefficient = 0\n",
    "\n",
    "    true_feature_importance = 0 #coefficient**2*(1-correlation**2)/(2+2*(coefficient-1)*correlation+coefficient**2+noise_variance) # 0.3249462191\n",
    "    \n",
    "    tmp = np.zeros((6,10))\n",
    "    counter = 0\n",
    "    for iteration in range(iterations):\n",
    "        if iteration%100 == 0:\n",
    "            print(iteration)\n",
    "        if iteration%(iterations/10) == 0:\n",
    "            cumulative_coverages = {'EIF': 0, 'Floodgate': 0}\n",
    "            cumulative_width_multipliers = {'EIF': 0, 'Floodgate': 0}\n",
    "\n",
    "            cumulative_floodgate_overshot = {'Floodgate': 0}\n",
    "            overshot_counter = 0\n",
    "            zero_LCB_counter = 0\n",
    "\n",
    "        # generate data\n",
    "        n = 2500\n",
    "        mean = np.zeros(4)\n",
    "        data_cov = correlated_cov_matrix(4,correlation)\n",
    "        X = np.random.multivariate_normal(mean,data_cov,n)\n",
    "        noise = np.random.normal(0,noise_variance**0.5,n)\n",
    "#         Y = 1.5*np.sin(X[:,0]*X[:,1]) + 0.25*np.exp(X[:,2]) + noise\n",
    "        Y = coefficient*X[:,0]+1*X[:,1]-1*X[:,2] +X[:,3] + noise\n",
    "\n",
    "        EIF_output = EIF(X,Y,confidence_level,true_feature_importance,base_learner)\n",
    "        Floodgate_output = Floodgate(X,Y,confidence_level,true_feature_importance,correlation,base_learner)\n",
    "\n",
    "        cumulative_coverages['EIF'] += EIF_output[0]\n",
    "        cumulative_coverages['Floodgate'] += Floodgate_output[0]\n",
    "\n",
    "        cumulative_width_multipliers['EIF'] += EIF_output[1]\n",
    "        cumulative_width_multipliers['Floodgate'] += Floodgate_output[1]\n",
    "\n",
    "        if Floodgate_output[2] == 0:\n",
    "            zero_LCB_counter += 1\n",
    "        if Floodgate_output[0] == 1:\n",
    "            cumulative_floodgate_overshot['Floodgate'] += true_feature_importance - Floodgate_output[2]\n",
    "            overshot_counter += 1\n",
    "        if iteration%(iterations/10) == (iterations/10)-1:\n",
    "            if overshot_counter != 0:\n",
    "                avg_overshot = cumulative_floodgate_overshot['Floodgate']/overshot_counter\n",
    "            else:\n",
    "                avg_overshot = np.nan\n",
    "            tmp[0,counter] = cumulative_coverages['EIF']/(iterations/10)\n",
    "            tmp[1,counter] = cumulative_coverages['Floodgate']/(iterations/10)\n",
    "            tmp[2,counter] = cumulative_width_multipliers['EIF']/(iterations/10)\n",
    "            tmp[3,counter] = cumulative_width_multipliers['Floodgate']/(iterations/10)\n",
    "            tmp[4,counter] = avg_overshot\n",
    "            tmp[5,counter] = zero_LCB_counter/(iterations/10)\n",
    "            counter += 1\n",
    "    mean = np.nanmean(tmp,axis=1)\n",
    "    se = np.nanstd(tmp,axis=1)*(1/9)**0.5\n",
    "    print(mean)\n",
    "    print(se)\n",
    "    \n",
    "    results_EIF_coverage_mean.append(mean[0])\n",
    "    results_Floodgate_coverage_mean.append(mean[1])\n",
    "    results_EIF_width_multiplier_mean.append(mean[2])\n",
    "    results_Floodgate_width_multiplier_mean.append(mean[3])\n",
    "    results_Floodgate_overshot_mean.append(mean[4])\n",
    "    results_zero_LCB_counter_mean.append(mean[5])\n",
    "    results_true_feature_importance.append(true_feature_importance)\n",
    "    \n",
    "    results_EIF_coverage_se.append(se[0])\n",
    "    results_Floodgate_coverage_se.append(se[1])\n",
    "    results_EIF_width_multiplier_se.append(se[2])\n",
    "    results_Floodgate_width_multiplier_se.append(se[3])\n",
    "    results_Floodgate_overshot_se.append(se[4])\n",
    "    results_zero_LCB_counter_se.append(se[5])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a108df",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_null = pd.DataFrame(data = {'EIF coverage': results_EIF_coverage_mean,'Floodgate coverage': results_Floodgate_coverage_mean,'EIF width multiplier': results_EIF_width_multiplier_mean,'Floodgate width multiplier': results_Floodgate_width_multiplier_mean,'Floodgate overshot': results_Floodgate_overshot_mean,'Floodgate zero LCB counter': results_zero_LCB_counter_mean,'true feature importance': results_true_feature_importance,'EIF coverage SE': results_EIF_coverage_se,'Floodgate coverage SE': results_Floodgate_coverage_se,'EIF width multiplier SE': results_EIF_width_multiplier_se,'Floodgate width multiplier SE': results_Floodgate_width_multiplier_se,'Floodgate overshot SE': results_Floodgate_overshot_se,'Floodgate zero LCB counter SE': results_zero_LCB_counter_se}, index=[0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ef395",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 'for' loop to evaluate performance over varied SNR results \n",
    "\n",
    "iterations = 1000\n",
    "confidence_level = 0.8 # needs to be greater than 0.5 to satisfy floodgate's conditions\n",
    "base_learner = RidgeCV(cv=5) #  RandomForestRegressor(n_estimators = 1000) RidgeCV(cv=5) LinearRegression()\n",
    "                                 # need to change base_learner in minipatch directly\n",
    "SNRs = [0,0.125,0.25,0.375,0.5,0.625,0.75,0.875,1,1.5,2,2.5,3.75,5,7.5,10,15,25,50,100]\n",
    "results_true_feature_importance = []\n",
    "\n",
    "results_EIF_coverage_mean = []\n",
    "results_Floodgate_coverage_mean = []\n",
    "results_EIF_width_multiplier_mean = []\n",
    "results_Floodgate_width_multiplier_mean = []\n",
    "results_Floodgate_overshot_mean = []\n",
    "results_zero_LCB_counter_mean = []\n",
    "\n",
    "results_EIF_coverage_se = []\n",
    "results_Floodgate_coverage_se = []\n",
    "results_EIF_width_multiplier_se = []\n",
    "results_Floodgate_width_multiplier_se = []\n",
    "results_Floodgate_overshot_se = []\n",
    "results_zero_LCB_counter_se = []\n",
    "\n",
    "for SNR in SNRs:\n",
    "    print(SNR)\n",
    "    \n",
    "    noise_variance = 1\n",
    "    correlation = 0\n",
    "    coefficient = SNR*noise_variance**0.5\n",
    "    \n",
    "    true_feature_importance = coefficient**2*(1-correlation**2)/(2+2*(coefficient-1)*correlation+coefficient**2+noise_variance) # 0.3249462191\n",
    "    \n",
    "    tmp = np.zeros((6,10))\n",
    "    counter = 0\n",
    "    for iteration in range(iterations):\n",
    "        if iteration%(iterations/10) == 0:\n",
    "            cumulative_coverages = {'EIF': 0, 'Floodgate': 0}\n",
    "            cumulative_width_multipliers = {'EIF': 0, 'Floodgate': 0}\n",
    "\n",
    "            cumulative_floodgate_overshot = {'Floodgate': 0}\n",
    "            overshot_counter = 0\n",
    "            zero_LCB_counter = 0\n",
    "\n",
    "        # generate data\n",
    "        n = 2500\n",
    "        mean = np.zeros(4)\n",
    "        data_cov = correlated_cov_matrix(4,correlation)\n",
    "        X = np.random.multivariate_normal(mean,data_cov,n)\n",
    "        noise = np.random.normal(0,noise_variance**0.5,n)\n",
    "#         Y = 1.5*np.sin(X[:,0]*X[:,1]) + 0.25*np.exp(X[:,2]) + noise\n",
    "        Y = coefficient*X[:,0]+1*X[:,1]-1*X[:,2] + noise\n",
    "\n",
    "        EIF_output = EIF(X,Y,confidence_level,true_feature_importance,base_learner)\n",
    "        Floodgate_output = Floodgate(X,Y,confidence_level,true_feature_importance,correlation,base_learner)\n",
    "\n",
    "        cumulative_coverages['EIF'] += EIF_output[0]\n",
    "        cumulative_coverages['Floodgate'] += Floodgate_output[0]\n",
    "\n",
    "        cumulative_width_multipliers['EIF'] += EIF_output[1]\n",
    "        cumulative_width_multipliers['Floodgate'] += Floodgate_output[1]\n",
    "\n",
    "        if Floodgate_output[2] == 0:\n",
    "            zero_LCB_counter += 1\n",
    "        if Floodgate_output[0] == 1:\n",
    "            cumulative_floodgate_overshot['Floodgate'] += true_feature_importance - Floodgate_output[2]\n",
    "            overshot_counter += 1\n",
    "        if iteration%(iterations/10) == (iterations/10)-1:\n",
    "            if overshot_counter != 0:\n",
    "                avg_overshot = cumulative_floodgate_overshot['Floodgate']/overshot_counter\n",
    "            else:\n",
    "                avg_overshot = np.nan\n",
    "            tmp[0,counter] = cumulative_coverages['EIF']/(iterations/10)\n",
    "            tmp[1,counter] = cumulative_coverages['Floodgate']/(iterations/10)\n",
    "            tmp[2,counter] = cumulative_width_multipliers['EIF']/(iterations/10)\n",
    "            tmp[3,counter] = cumulative_width_multipliers['Floodgate']/(iterations/10)\n",
    "            tmp[4,counter] = avg_overshot\n",
    "            tmp[5,counter] = zero_LCB_counter/(iterations/10)\n",
    "            counter += 1\n",
    "    mean = np.nanmean(tmp,axis=1)\n",
    "    se = np.nanstd(tmp,axis=1)*(1/9)**0.5\n",
    "   \n",
    "    results_EIF_coverage_mean.append(mean[0])\n",
    "    results_Floodgate_coverage_mean.append(mean[1])\n",
    "    results_EIF_width_multiplier_mean.append(mean[2])\n",
    "    results_Floodgate_width_multiplier_mean.append(mean[3])\n",
    "    results_Floodgate_overshot_mean.append(mean[4])\n",
    "    results_zero_LCB_counter_mean.append(mean[5])\n",
    "    results_true_feature_importance.append(true_feature_importance)\n",
    "    \n",
    "    results_EIF_coverage_se.append(se[0])\n",
    "    results_Floodgate_coverage_se.append(se[1])\n",
    "    results_EIF_width_multiplier_se.append(se[2])\n",
    "    results_Floodgate_width_multiplier_se.append(se[3])\n",
    "    results_Floodgate_overshot_se.append(se[4])\n",
    "    results_zero_LCB_counter_se.append(se[5])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eb82ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_SNR_1 = pd.DataFrame(data = {'EIF coverage': results_EIF_coverage_mean,'Floodgate coverage': results_Floodgate_coverage_mean,'EIF width multiplier': results_EIF_width_multiplier_mean,'Floodgate width multiplier': results_Floodgate_width_multiplier_mean,'Floodgate overshot': results_Floodgate_overshot_mean,'Floodgate zero LCB counter': results_zero_LCB_counter_mean,'true feature importance': results_true_feature_importance,'EIF coverage SE': results_EIF_coverage_se,'Floodgate coverage SE': results_Floodgate_coverage_se,'EIF width multiplier SE': results_EIF_width_multiplier_se,'Floodgate width multiplier SE': results_Floodgate_width_multiplier_se,'Floodgate overshot SE': results_Floodgate_overshot_se,'Floodgate zero LCB counter SE': results_zero_LCB_counter_se}, index=[0,0.125,0.25,0.375,0.5,0.625,0.75,0.875,1,1.5,2,2.5,3.75,5,7.5,10,15,25,50,100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139512ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 'for' loop to evaluate performance over varied SNR results \n",
    "\n",
    "iterations = 1000\n",
    "confidence_level = 0.8 # needs to be greater than 0.5 to satisfy floodgate's conditions\n",
    "base_learner = RidgeCV(cv=5) #  RandomForestRegressor(n_estimators = 1000) RidgeCV(cv=5) LinearRegression()\n",
    "                                 # need to change base_learner in minipatch directly\n",
    "SNRs = [0.05,0.1,0.15,0.2,0.25,0.375,0.5,0.625,0.75,0.875,1,1.25,1.5,1.75,2,2.5,3.75,5,7.5,10]\n",
    "\n",
    "results_true_feature_importance = []\n",
    "\n",
    "results_EIF_coverage_mean = []\n",
    "results_Floodgate_coverage_mean = []\n",
    "results_EIF_width_multiplier_mean = []\n",
    "results_Floodgate_width_multiplier_mean = []\n",
    "results_Floodgate_overshot_mean = []\n",
    "results_zero_LCB_counter_mean = []\n",
    "\n",
    "results_EIF_coverage_se = []\n",
    "results_Floodgate_coverage_se = []\n",
    "results_EIF_width_multiplier_se = []\n",
    "results_Floodgate_width_multiplier_se = []\n",
    "results_Floodgate_overshot_se = []\n",
    "results_zero_LCB_counter_se = []\n",
    "\n",
    "for SNR in SNRs:\n",
    "    print(SNR)\n",
    "    \n",
    "    correlation = 0\n",
    "    coefficient = 1\n",
    "    noise_variance = (coefficient/SNR)**2\n",
    "#     correlation = 0.15 # need abs(correlation) < 2/(1+root(5)) = 0.618 for positive definite correlation matrix\n",
    "\n",
    "    true_feature_importance = coefficient**2*(1-correlation**2)/(2+2*(coefficient-1)*correlation+coefficient**2+noise_variance) # 0.3249462191\n",
    "    \n",
    "    tmp = np.zeros((6,10))\n",
    "    counter = 0\n",
    "    for iteration in range(iterations):\n",
    "        if iteration%(iterations/10) == 0:\n",
    "            cumulative_coverages = {'EIF': 0, 'Floodgate': 0}\n",
    "            cumulative_width_multipliers = {'EIF': 0, 'Floodgate': 0}\n",
    "\n",
    "            cumulative_floodgate_overshot = {'Floodgate': 0}\n",
    "            overshot_counter = 0\n",
    "            zero_LCB_counter = 0\n",
    "\n",
    "        # generate data\n",
    "        n = 2500\n",
    "        mean = np.zeros(4)\n",
    "        data_cov = correlated_cov_matrix(4,correlation)\n",
    "        X = np.random.multivariate_normal(mean,data_cov,n)\n",
    "        noise = np.random.normal(0,noise_variance**0.5,n)\n",
    "#         Y = 1.5*np.sin(X[:,0]*X[:,1]) + 0.25*np.exp(X[:,2]) + noise\n",
    "        Y = coefficient*X[:,0]+1*X[:,1]-1*X[:,2] + noise\n",
    "\n",
    "        EIF_output = EIF(X,Y,confidence_level,true_feature_importance,base_learner)\n",
    "        Floodgate_output = Floodgate(X,Y,confidence_level,true_feature_importance,correlation,base_learner)\n",
    "\n",
    "        cumulative_coverages['EIF'] += EIF_output[0]\n",
    "        cumulative_coverages['Floodgate'] += Floodgate_output[0]\n",
    "\n",
    "        cumulative_width_multipliers['EIF'] += EIF_output[1]\n",
    "        cumulative_width_multipliers['Floodgate'] += Floodgate_output[1]\n",
    "\n",
    "        if Floodgate_output[2] == 0:\n",
    "            zero_LCB_counter += 1\n",
    "        if Floodgate_output[0] == 1:\n",
    "            cumulative_floodgate_overshot['Floodgate'] += true_feature_importance - Floodgate_output[2]\n",
    "            overshot_counter += 1\n",
    "        if iteration%(iterations/10) == (iterations/10)-1:\n",
    "            if overshot_counter != 0:\n",
    "                avg_overshot = cumulative_floodgate_overshot['Floodgate']/overshot_counter\n",
    "            else:\n",
    "                avg_overshot = np.nan\n",
    "            tmp[0,counter] = cumulative_coverages['EIF']/(iterations/10)\n",
    "            tmp[1,counter] = cumulative_coverages['Floodgate']/(iterations/10)\n",
    "            tmp[2,counter] = cumulative_width_multipliers['EIF']/(iterations/10)\n",
    "            tmp[3,counter] = cumulative_width_multipliers['Floodgate']/(iterations/10)\n",
    "            tmp[4,counter] = avg_overshot\n",
    "            tmp[5,counter] = zero_LCB_counter/(iterations/10)\n",
    "            counter += 1\n",
    "    mean = np.nanmean(tmp,axis=1)\n",
    "    se = np.nanstd(tmp,axis=1)*(1/9)**0.5\n",
    "   \n",
    "    results_EIF_coverage_mean.append(mean[0])\n",
    "    results_Floodgate_coverage_mean.append(mean[1])\n",
    "    results_EIF_width_multiplier_mean.append(mean[2])\n",
    "    results_Floodgate_width_multiplier_mean.append(mean[3])\n",
    "    results_Floodgate_overshot_mean.append(mean[4])\n",
    "    results_zero_LCB_counter_mean.append(mean[5])\n",
    "    results_true_feature_importance.append(true_feature_importance)\n",
    "    \n",
    "    results_EIF_coverage_se.append(se[0])\n",
    "    results_Floodgate_coverage_se.append(se[1])\n",
    "    results_EIF_width_multiplier_se.append(se[2])\n",
    "    results_Floodgate_width_multiplier_se.append(se[3])\n",
    "    results_Floodgate_overshot_se.append(se[4])\n",
    "    results_zero_LCB_counter_se.append(se[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_SNR_2 = pd.DataFrame(data = {'EIF coverage': results_EIF_coverage_mean,'Floodgate coverage': results_Floodgate_coverage_mean,'EIF width multiplier': results_EIF_width_multiplier_mean,'Floodgate width multiplier': results_Floodgate_width_multiplier_mean,'Floodgate overshot': results_Floodgate_overshot_mean,'Floodgate zero LCB counter': results_zero_LCB_counter_mean,'true feature importance': results_true_feature_importance,'EIF coverage SE': results_EIF_coverage_se,'Floodgate coverage SE': results_Floodgate_coverage_se,'EIF width multiplier SE': results_EIF_width_multiplier_se,'Floodgate width multiplier SE': results_Floodgate_width_multiplier_se,'Floodgate overshot SE': results_Floodgate_overshot_se,'Floodgate zero LCB counter SE': results_zero_LCB_counter_se}, index=[0.05,0.1,0.15,0.2,0.25,0.375,0.5,0.625,0.75,0.875,1,1.25,1.5,1.75,2,2.5,3.75,5,7.5,10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display coverage/width multiplier results\n",
    "display = 'width multiplier'\n",
    "df = results_df_SNR_2\n",
    "xlabel = 'SNR'\n",
    "\n",
    "if display == 'coverage':\n",
    "    label = 'coverage'\n",
    "    plt.plot([-10,100],[0.8 for i in range(2)],label = 'nominal rate',color='r')\n",
    "elif display == 'width multiplier':\n",
    "    label = 'width mult'\n",
    "    plt.errorbar(df.index,df['Floodgate overshot'], df['Floodgate overshot SE']*1.96,label = 'Floodgate overshot',color = 'y')\n",
    "plt.errorbar(df.index,df['EIF '+display], df['EIF '+display + ' SE']*1.96,label = 'EIF '+ label)\n",
    "plt.errorbar(df.index,df['Floodgate '+display],df['Floodgate ' +display + ' SE']*1.96,label = 'Floodgate ' + label,color='g')\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(display)\n",
    "\n",
    "plt.xlim(1,10.1)\n",
    "plt.ylim(0.725,0.925)\n",
    "plt.xticks([1+i for i in range(10)])\n",
    "plt.yticks([0.725+0.025*i for i in range(9)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69d2bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_copy = results_df\n",
    "results_df_null_copy = results_df_null\n",
    "results_df_SNR_1_copy = results_df_SNR_1\n",
    "results_df_SNR_2_copy = results_df_SNR_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7faf7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
